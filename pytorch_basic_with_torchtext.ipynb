{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch có 2 hàm để làm việc với dữ liệu là : torch.utils.data.DataLoader và torch.utils.data.DataSet. Hàm dataset sẽ các ví dụ và nhãn của chúng. DataLoader sẽ áp dụng vòng lặp lên Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngoài ra, pytorch cung cấp các thư viện có sẵn những bộ dữ liệu có sẵn ứng với từng lĩnh vực trong AI như xử lý ngôn ngữ tự nhiên, thị giác máy tính,... như torchtext, torchvision, torchaudio. Ví dụ sẽ sử dụng bộ dữ liệu IMDB để phân lớp văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: portalocker in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from portalocker) (305.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install portalocker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchtext in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torchtext) (4.66.1)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torchtext) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.0 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torchtext) (2.1.0)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torchtext) (1.24.3)\n",
      "Requirement already satisfied: torchdata==0.7.0 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torchtext) (0.7.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torch==2.1.0->torchtext) (3.12.2)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torch==2.1.0->torchtext) (4.5.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torch==2.1.0->torchtext) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torch==2.1.0->torchtext) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torch==2.1.0->torchtext) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torch==2.1.0->torchtext) (2023.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from torchdata==0.7.0->torchtext) (1.26.16)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from requests->torchtext) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from requests->torchtext) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from requests->torchtext) (2023.7.22)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from tqdm->torchtext) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from jinja2->torch==2.1.0->torchtext) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from sympy->torch==2.1.0->torchtext) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dưới đây là một ví dụ về xử lý dữ liệu NLP điển hình với tokenizer và vocab. Bước đầu tiên là xây dựng bộ từ điển với tập dữ liệu huấn luyện. Ở đây, chúng tôi sử dụng hàm dựng sẵn build_vocab_from_iterator để chấp nhận trình lặp mang lại danh sách hoặc trình lặp tokens. Người dùng cũng có thể chuyển bất kỳ ký hiệu đặc biệt nào để thêm vào từ vựng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "train_iter = datasets.IMDB(\n",
    "    root=\"data\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "test_iter = datasets.IMDB(\n",
    "    root=\"data\",\n",
    "    split=\"test\"\n",
    "    \n",
    ")\n",
    "train_iter\n",
    "test_iter\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) #add special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131, 9, 40, 464]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn bị quy trình xử lý văn bản với tokenzier và vocab. Các text pipelines và label pipelines sẽ được sử dụng để xử lý các chuỗi dữ liệu thô từ các vòng lặp của dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text pipeline chuyển đổi một chuỗi văn bản thành danh sách các số nguyên dựa trên bảng tra cứu được xác định trong vocab. Label pipeline chuyển đổi nhãn thành số nguyên. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')\n",
    "label_pipeline('10')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader được khuyến nghị cho người dùng PyTorch. Nó hoạt động với tập dữ liệu kiểu mapping, triển khai các giao thức getitem() và len() và thể hiện mapping từ các chỉ mục/khóa đến mẫu dữ liệu. Nó cũng hoạt động với một tập dữ liệu có thể lặp lại với đối số xáo trộn(shuffle) là False.\n",
    "\n",
    "Trước khi gửi đến mô hình, hàm collate_fn hoạt động trên một loạt mẫu được tạo từ DataLoader. Đầu vào của collate_fn là một batch dữ liệu có batch_size trong DataLoader và collate_fn xử lý chúng theo quy trình xử lý dữ liệu đã khai báo trước đó. Hãy chú ý ở đây và đảm bảo rằng collate_fn được khai báo là def cấp cao nhất. Điều này đảm bảo rằng chức năng này có sẵn trong mỗi công nhân.\n",
    "\n",
    "Trong ví dụ này, các văn bản trong dữ liệu đầu vào theo batch ban đầu được đóng gói thành một danh sách và được nối thành một tensor đơn cho đầu vào của nn.EmbeddingBag. Offset là một tensor của các dấu phân cách để biểu thị chỉ số bắt đầu của chuỗi riêng lẻ trong tensor văn bản. Nhãn là một tensor lưu nhãn của các mục văn bản riêng lẻ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch có thể huấn luyện với CPU và GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0+cpu'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng mô hình\n",
    "Mô hình này bao gồm lớp nn.EmbeddingBag cộng với lớp tuyến tính cho mục đích phân loại. nn.EmbeddingBag với chế độ mặc định là “trung bình” sẽ tính giá trị trung bình của một “túi” các phần embedding. Mặc dù các mục nhập văn bản ở đây có độ dài khác nhau, mô-đun nn.EmbeddingBag không yêu cầu phần đệm ở đây vì độ dài văn bản được lưu theo độ lệch.\n",
    "\n",
    "Ngoài ra, vì nn.EmbeddingBag tích lũy nhanh chóng mức trung bình trên các phần nhúng nên nn.EmbeddingBag có thể nâng cao hiệu suất và hiệu quả bộ nhớ để xử lý một chuỗi các tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedd_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=embedd_dim) #Embedding layer\n",
    "        self.linear = nn.Linear(in_features=embedd_dim, out_features=num_class) #Linear Layer\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self): # initialize weight metris\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange) #embedding weight\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange) #linear weight\n",
    "        self.linear.bias.data.zero_() #bias\n",
    "        \n",
    "    def forward(self, text, offsets):\n",
    "        x = self.embedding(text, offsets)\n",
    "        output = self.linear(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (embedding): EmbeddingBag(100683, 64, mode='mean')\n",
       "  (linear): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_size = 64\n",
    "model = NeuralNetwork(vocab_size=vocab_size, embedd_dim=embedding_size, num_class=num_class)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thuật toán tối ưu và hàm mất mát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr) \n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train() #mode of model\n",
    "    total_acc = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    log_interval = 500\n",
    "    \n",
    "    for batch, (label, text, offsets) in enumerate(dataloader):\n",
    "        #model prediction\n",
    "        pred = model(text, offsets)\n",
    "        \n",
    "        #compute loss\n",
    "        loss = loss_function(pred, label)\n",
    "        \n",
    "        #backprobagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_acc += (pred.argmax(1)==label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, batch, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước test mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    model.eval() #mode of model\n",
    "    \n",
    "    total_acc = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (label, text, offsets) in enumerate(dataloader):        \n",
    "                pred = model(text, offsets)\n",
    "                \n",
    "                loss = loss_function(pred, label)\n",
    "                total_acc += (pred.argmax(1) == label).sum().item()\n",
    "                total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quá trình huấn luyện được thực hiện qua nhiều lần lặp (epoch). Trong mỗi epoch, mô hình sẽ điều chỉnh các tham số để đưa ra dự đoán tốt hơn. Chúng tôi in độ chính xác và độ mất mát của mô hình tại mỗi epoch; chúng tôi muốn thấy độ chính xác tăng lên và độ mất mát giảm dần theo từng epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | valid accuracy    0.805 \n",
      "-----------------------------------------------------------\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | valid accuracy    0.825 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 2  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1) #adjust learning rate throught each epoch\n",
    "total_accu = None\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.20)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(train_dataloader)\n",
    "    accu_val = test(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thử nghiệm mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4437,  0.3866]])\n",
      "positive\n"
     ]
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_size = 64\n",
    "\n",
    "model = NeuralNetwork(vocab_size=vocab_size, embedd_dim=embedding_size, num_class=num_class)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "imdb_label = {0 : 'negative', 1 : 'positive'}\n",
    "\n",
    "model = model.to(device)\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        print(output)\n",
    "        return output.argmax(1).item()\n",
    "\n",
    "text = \"This move is good\"\n",
    "\n",
    "print(imdb_label[predict(text, text_pipeline)])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
