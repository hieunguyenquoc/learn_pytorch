{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch có 2 hàm để làm việc với dữ liệu là : torch.utils.data.DataLoader và torch.utils.data.DataSet. Hàm dataset sẽ các ví dụ và nhãn của chúng. DataLoader sẽ áp dụng vòng lặp lên Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\python3.8\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ngoài ra, pytorch cung cấp các thư viện có sẵn những bộ dữ liệu có sẵn ứng với từng lĩnh vực trong AI như xử lý ngôn ngữ tự nhiên, thị giác máy tính,... như torchtext, torchvision, torchaudio. Ví dụ sẽ sử dụng bộ dữ liệu IMDB để phân lớp văn bản"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: portalocker in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (2.8.2)\n",
      "Requirement already satisfied: pywin32>=226 in d:\\anaconda\\envs\\python3.8\\lib\\site-packages (from portalocker) (305.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (d:\\anaconda\\envs\\python3.8\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\anaconda\\envs\\python3.8\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install portalocker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dưới đây là một ví dụ về xử lý dữ liệu NLP điển hình với tokenizer và vocab. Bước đầu tiên là xây dựng bộ từ điển với tập dữ liệu huấn luyện. Ở đây, chúng tôi sử dụng hàm dựng sẵn build_vocab_from_iterator để chấp nhận trình lặp mang lại danh sách hoặc trình lặp tokens. Người dùng cũng có thể chuyển bất kỳ ký hiệu đặc biệt nào để thêm vào từ vựng."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import datasets\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "train_iter = datasets.IMDB(\n",
    "    root=\"data\",\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "test_iter = datasets.IMDB(\n",
    "    root=\"data\",\n",
    "    split=\"test\"\n",
    "    \n",
    ")\n",
    "train_iter\n",
    "test_iter\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) #add special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131, 9, 40, 464]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chuẩn bị quy trình xử lý văn bản với tokenzier và vocab. Các text pipelines và label pipelines sẽ được sử dụng để xử lý các chuỗi dữ liệu thô từ các vòng lặp của dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text pipeline chuyển đổi một chuỗi văn bản thành danh sách các số nguyên dựa trên bảng tra cứu được xác định trong vocab. Label pipeline chuyển đổi nhãn thành số nguyên. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is the an example')\n",
    "label_pipeline('10')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.utils.data.DataLoader được khuyến nghị cho người dùng PyTorch. Nó hoạt động với tập dữ liệu kiểu mapping, triển khai các giao thức getitem() và len() và thể hiện mapping từ các chỉ mục/khóa đến mẫu dữ liệu. Nó cũng hoạt động với một tập dữ liệu có thể lặp lại với đối số xáo trộn(shuffle) là False.\n",
    "\n",
    "Trước khi gửi đến mô hình, hàm collate_fn hoạt động trên một loạt mẫu được tạo từ DataLoader. Đầu vào của collate_fn là một batch dữ liệu có batch_size trong DataLoader và collate_fn xử lý chúng theo quy trình xử lý dữ liệu đã khai báo trước đó. Hãy chú ý ở đây và đảm bảo rằng collate_fn được khai báo là def cấp cao nhất. Điều này đảm bảo rằng chức năng này có sẵn trong mỗi công nhân.\n",
    "\n",
    "Trong ví dụ này, các văn bản trong dữ liệu đầu vào theo batch ban đầu được đóng gói thành một danh sách và được nối thành một tensor đơn cho đầu vào của nn.EmbeddingBag. Offset là một tensor của các dấu phân cách để biểu thị chỉ số bắt đầu của chuỗi riêng lẻ trong tensor văn bản. Nhãn là một tensor lưu nhãn của các mục văn bản riêng lẻ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for _label, _text in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch có thể huấn luyện với CPU và GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng mô hình\n",
    "Mô hình này bao gồm lớp nn.EmbeddingBag cộng với lớp tuyến tính cho mục đích phân loại. nn.EmbeddingBag với chế độ mặc định là “trung bình” sẽ tính giá trị trung bình của một “túi” các phần embedding. Mặc dù các mục nhập văn bản ở đây có độ dài khác nhau, mô-đun nn.EmbeddingBag không yêu cầu phần đệm ở đây vì độ dài văn bản được lưu theo độ lệch.\n",
    "\n",
    "Ngoài ra, vì nn.EmbeddingBag tích lũy nhanh chóng mức trung bình trên các phần nhúng nên nn.EmbeddingBag có thể nâng cao hiệu suất và hiệu quả bộ nhớ để xử lý một chuỗi các tensor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size, embedd_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(num_embeddings=vocab_size, embedding_dim=embedd_dim) #Embedding layer\n",
    "        self.linear = nn.Linear(in_features=embedd_dim, out_features=num_class) #Linear Layer\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self): # initialize weight metris\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output = self.linear(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\python3.8\\lib\\site-packages\\torch\\nn\\init.py:388: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (embedding): EmbeddingBag(100683, 64, mode=mean)\n",
       "  (linear): Linear(in_features=64, out_features=0, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_size = 64\n",
    "model = NeuralNetwork(vocab_size=vocab_size, embedd_dim=embedding_size, num_class=num_class)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thuật toán tối ưu và hàm mất mát"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader):\n",
    "    model.train() #mode of model\n",
    "    total_acc = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    log_interval = 500\n",
    "    \n",
    "    for batch, (label, text, offsets) in enumerate(dataloader):\n",
    "        #model prediction\n",
    "        pred = model(text, offsets)\n",
    "        \n",
    "        #compute loss\n",
    "        loss = loss_function(pred, label)\n",
    "        \n",
    "        #backprobagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_acc += (pred.argmax(1)==label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            print(\n",
    "                \"| epoch {:3d} | {:5d}/{:5d} batches \"\n",
    "                \"| accuracy {:8.3f}\".format(\n",
    "                    epoch, batch, len(dataloader), total_acc / total_count\n",
    "                )\n",
    "            )\n",
    "            total_acc, total_count = 0, 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước test mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader):\n",
    "    model.eval() #mode of model\n",
    "    \n",
    "    total_acc = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (label, text, offsets) in enumerate(dataloader):        \n",
    "                pred = model(text)\n",
    "                \n",
    "                loss += loss_function(pred, label)\n",
    "                total_acc += (pred.argmax(1) == label).sum().item()\n",
    "                total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quá trình huấn luyện được thực hiện qua nhiều lần lặp (epoch). Trong mỗi epoch, mô hình sẽ điều chỉnh các tham số để đưa ra dự đoán tốt hơn. Chúng tôi in độ chính xác và độ mất mát của mô hình tại mỗi epoch; chúng tôi muốn thấy độ chính xác tăng lên và độ mất mát giảm dần theo từng epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The hash of d:\\Learn_at_free_time\\.data\\IMDB\\aclImdb_v1.tar.gz does not match. Delete the file manually and retry.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mStepLR(optimizer, \u001b[39m1.0\u001b[39m, gamma\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m)\n\u001b[0;32m     11\u001b[0m total_accu \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m train_iter, test_iter \u001b[39m=\u001b[39m datasets\u001b[39m.\u001b[39;49mIMDB()\n\u001b[0;32m     13\u001b[0m train_dataset \u001b[39m=\u001b[39m to_map_style_dataset(train_iter)\n\u001b[0;32m     14\u001b[0m test_dataset \u001b[39m=\u001b[39m to_map_style_dataset(test_iter)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\python3.8\\lib\\site-packages\\torchtext\\data\\datasets_utils.py:257\u001b[0m, in \u001b[0;36m_create_dataset_directory.<locals>.decorator.<locals>.wrapper\u001b[1;34m(root, *args, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(new_root):\n\u001b[0;32m    256\u001b[0m     os\u001b[39m.\u001b[39mmakedirs(new_root)\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m func(root\u001b[39m=\u001b[39;49mnew_root, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\python3.8\\lib\\site-packages\\torchtext\\data\\datasets_utils.py:219\u001b[0m, in \u001b[0;36m_wrap_split_argument_with_fn.<locals>.new_fn\u001b[1;34m(root, split, **kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[0;32m    218\u001b[0m \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m _check_default_set(split, splits, fn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m):\n\u001b[1;32m--> 219\u001b[0m     result\u001b[39m.\u001b[39mappend(fn(root, item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[0;32m    220\u001b[0m \u001b[39mreturn\u001b[39;00m _wrap_datasets(\u001b[39mtuple\u001b[39m(result), split)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\python3.8\\lib\\site-packages\\torchtext\\datasets\\imdb.py:34\u001b[0m, in \u001b[0;36mIMDB\u001b[1;34m(root, split)\u001b[0m\n\u001b[0;32m     32\u001b[0m                 label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mpos\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m fname \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mneg\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     33\u001b[0m                 \u001b[39myield\u001b[39;00m label, f\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 34\u001b[0m dataset_tar \u001b[39m=\u001b[39m download_from_url(URL, root\u001b[39m=\u001b[39;49mroot,\n\u001b[0;32m     35\u001b[0m                                 hash_value\u001b[39m=\u001b[39;49mMD5, hash_type\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmd5\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     36\u001b[0m extracted_files \u001b[39m=\u001b[39m extract_archive(dataset_tar)\n\u001b[0;32m     37\u001b[0m iterator \u001b[39m=\u001b[39m generate_imdb_data(split, extracted_files)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\python3.8\\lib\\site-packages\\torchtext\\utils.py:116\u001b[0m, in \u001b[0;36mdownload_from_url\u001b[1;34m(url, path, root, overwrite, hash_value, hash_type)\u001b[0m\n\u001b[0;32m    114\u001b[0m     logging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m already exists.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m path)\n\u001b[0;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m overwrite:\n\u001b[1;32m--> 116\u001b[0m         _check_hash(path)\n\u001b[0;32m    117\u001b[0m         \u001b[39mreturn\u001b[39;00m path\n\u001b[0;32m    119\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdrive.google.com\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m url:\n",
      "File \u001b[1;32md:\\anaconda\\envs\\python3.8\\lib\\site-packages\\torchtext\\utils.py:65\u001b[0m, in \u001b[0;36mdownload_from_url.<locals>._check_hash\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(path, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file_obj:\n\u001b[0;32m     64\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m validate_file(file_obj, hash_value, hash_type):\n\u001b[1;32m---> 65\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe hash of \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m does not match. Delete the file manually and retry.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mabspath(path)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The hash of d:\\Learn_at_free_time\\.data\\IMDB\\aclImdb_v1.tar.gz does not match. Delete the file manually and retry."
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "# Hyperparameters\n",
    "EPOCHS = 2  # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64  # batch size for training\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = datasets.IMDB()\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.20)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(train_dataloader)\n",
    "    accu_val = test(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print(\"-\" * 59)\n",
    "    print(\n",
    "        \"| end of epoch {:3d} | \"\n",
    "        \"valid accuracy {:8.3f} \".format(\n",
    "            epoch, accu_val\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 59)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
